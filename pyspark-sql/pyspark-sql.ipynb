{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b6fe9bf2-6094-4e51-a2c8-0fbce20dc4bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T15:48:26.158558Z",
     "iopub.status.busy": "2022-05-19T15:48:26.158336Z",
     "iopub.status.idle": "2022-05-19T15:48:26.214230Z",
     "shell.execute_reply": "2022-05-19T15:48:26.213648Z",
     "shell.execute_reply.started": "2022-05-19T15:48:26.158534Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4eec26246b4686bf40969d8a702542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9ada48ae-4129-45c0-92a9-e809104599df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T15:48:29.128676Z",
     "iopub.status.busy": "2022-05-19T15:48:29.128331Z",
     "iopub.status.idle": "2022-05-19T15:48:29.200347Z",
     "shell.execute_reply": "2022-05-19T15:48:29.199580Z",
     "shell.execute_reply.started": "2022-05-19T15:48:29.128637Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a77dfaaacf4c8a8c092b51e9e5bd29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jobsSchema = StructType([\n",
    "    StructField(\"Job_Role\", StringType()),\n",
    "    StructField(\"Company\", StringType()),\n",
    "    StructField(\"Location\", StringType()),\n",
    "    StructField(\"Job_Experience\", StringType()),\n",
    "    StructField(\"Skills_&_Description\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "16b13a53-4f86-44c2-9663-6136dca0594a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T15:48:31.837955Z",
     "iopub.status.busy": "2022-05-19T15:48:31.837725Z",
     "iopub.status.idle": "2022-05-19T15:48:32.133538Z",
     "shell.execute_reply": "2022-05-19T15:48:32.132745Z",
     "shell.execute_reply.started": "2022-05-19T15:48:31.837932Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1eabf8d61c416bb7b3c15ec08b9862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "|      upgrad|\n",
      "+------------+"
     ]
    }
   ],
   "source": [
    "# Create DB and use it.\n",
    "spark.sql(\"drop database if EXISTS upgrad\")\n",
    "spark.sql(\"create database upgrad\")\n",
    "spark.sql(\"use upgrad\")\n",
    "databasesDF = spark.sql(\"show databases\")\n",
    "databasesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cc684346-2f7a-46a5-883e-514c90e3cbb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T15:52:08.017159Z",
     "iopub.status.busy": "2022-05-19T15:52:08.016926Z",
     "iopub.status.idle": "2022-05-19T15:52:19.368255Z",
     "shell.execute_reply": "2022-05-19T15:52:19.367506Z",
     "shell.execute_reply.started": "2022-05-19T15:52:08.017135Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0722e54c70e34a3cae2a9b68890111c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# header = true, to it will use the first row in the csv file as the dataframe's column names\n",
    "jobsDf = spark.read.csv(\"s3://kaggle-dataset-pyspark/naukri_data_science_jobs_india.csv\", jobsSchema)\n",
    "jobsDf.write.mode(\"overwrite\").saveAsTable(\"naukri_ds_jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a04b29e9-fee3-4b19-b8dc-43423c41694d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T16:18:24.976582Z",
     "iopub.status.busy": "2022-05-19T16:18:24.976346Z",
     "iopub.status.idle": "2022-05-19T16:18:34.307414Z",
     "shell.execute_reply": "2022-05-19T16:18:34.306650Z",
     "shell.execute_reply.started": "2022-05-19T16:18:24.976559Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48147958f29e42b3bba0749fe01cd2b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-------------------+--------------+--------------------+\n",
      "|            Job_Role|Company|           Location|Job_Experience|Skills_&_Description|\n",
      "+--------------------+-------+-------------------+--------------+--------------------+\n",
      "|Data Scientist: A...|    IBM|Bangalore/Bengaluru|        06-Nov|Scheduling, big d...|\n",
      "|Staff Data Scientist|Walmart|Bangalore/Bengaluru|        04-Aug|IT Skills, Data S...|\n",
      "|Data Scientist-Cl...|    IBM|             Mumbai|        05-Oct|Consulting, Machi...|\n",
      "|Senior Data Scien...|Walmart|Bangalore/Bengaluru|        05-Oct|IT Skills, Testin...|\n",
      "|Cognitive/AI Seni...|    IBM|Bangalore/Bengaluru|        04-Aug|Automation, Consu...|\n",
      "+--------------------+-------+-------------------+--------------+--------------------+"
     ]
    }
   ],
   "source": [
    "# SELECT DATA from the naukri_ds_jobs.\n",
    "all_results = spark.sql(\"SELECT * from upgrad.naukri_ds_jobs limit 5\")\n",
    "all_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "301fe607-f2e4-4d2b-b84a-67738e97c49e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T16:21:49.488757Z",
     "iopub.status.busy": "2022-05-19T16:21:49.488530Z",
     "iopub.status.idle": "2022-05-19T16:21:58.847806Z",
     "shell.execute_reply": "2022-05-19T16:21:58.846860Z",
     "shell.execute_reply.started": "2022-05-19T16:21:49.488732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b90129f22c4360843518c198c2a215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-------------------+--------------+--------------------+\n",
      "|            Job_Role|Company|           Location|Job_Experience|Skills_&_Description|\n",
      "+--------------------+-------+-------------------+--------------+--------------------+\n",
      "|Data Scientist: A...|    IBM|Bangalore/Bengaluru|        06-Nov|Scheduling, big d...|\n",
      "|Data Scientist-Cl...|    IBM|             Mumbai|        05-Oct|Consulting, Machi...|\n",
      "|Cognitive/AI Seni...|    IBM|Bangalore/Bengaluru|        04-Aug|Automation, Consu...|\n",
      "|Mid Level Data Sc...|    IBM|          Ahmedabad|        05-Aug|data science, Mac...|\n",
      "|      Data Scientist|    IBM|       Kochi/Cochin|        04-Aug|Software design, ...|\n",
      "|Data Scientist: A...|    IBM|Bangalore/Bengaluru|        02-May|Interpersonal ski...|\n",
      "|Data Scientist: A...|    IBM|Bangalore/Bengaluru|        03-Jul|Data migration, M...|\n",
      "|Data Scientist: A...|    IBM|Bangalore/Bengaluru|        03-Jul|Data migration, M...|\n",
      "|      Data Scientist|    IBM|Bangalore/Bengaluru|        08-Dec|Unix, VMware, Net...|\n",
      "|      Data Scientist|    IBM|       Kochi/Cochin|        08-Dec|Computer science,...|\n",
      "|      Data Scientist|    IBM|Bangalore/Bengaluru|        08-Dec|Consulting, Predi...|\n",
      "|Data Scientist: A...|    IBM|Bangalore/Bengaluru|        04-Aug|IVR, Telecommunic...|\n",
      "|Data Scientist: A...|    IBM|Bangalore/Bengaluru|        03-Jul|Enterprise applic...|\n",
      "|Data Scientist: A...|    IBM|Bangalore/Bengaluru|        04-Aug|IVR, Telecommunic...|\n",
      "|Data Scientist: A...|    IBM|Bangalore/Bengaluru|        03-Jul|Enterprise applic...|\n",
      "|Data Scientist: A...|    IBM|Bangalore/Bengaluru|        03-Jul|Enterprise applic...|\n",
      "|Data Scientist: A...|    IBM|Bangalore/Bengaluru|        03-Jul|Enterprise applic...|\n",
      "|      Data Scientist|    IBM|Bangalore/Bengaluru|        03-Jun|Coding, Debugging...|\n",
      "|      Data Scientist|    IBM|Bangalore/Bengaluru|        03-Jun|Coding, Debugging...|\n",
      "|Data Scientist: A...|    IBM|Bangalore/Bengaluru|        05-Oct|Training, advance...|\n",
      "+--------------------+-------+-------------------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "IBM count is: 273"
     ]
    }
   ],
   "source": [
    "# SELECT DATA from the naukri_ds_jobs where company name = \"IBM\"\n",
    "ibm_company_results = spark.sql(\"SELECT * from upgrad.naukri_ds_jobs where company = 'IBM'\")\n",
    "ibm_company_results.show()\n",
    "print(\"IBM count is: {}\".format(ibm_company_results.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c33a37a7-4c69-40b6-93b3-8f53b71c95e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-19T16:31:16.882455Z",
     "iopub.status.busy": "2022-05-19T16:31:16.882101Z",
     "iopub.status.idle": "2022-05-19T16:31:28.279764Z",
     "shell.execute_reply": "2022-05-19T16:31:28.279040Z",
     "shell.execute_reply.started": "2022-05-19T16:31:16.882413Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08609d6974044a0f91aa9f5d343686f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of companies having presence in Bangalore is: 4687\n",
      "+--------------------+-------+-------------------+--------------+--------------------+\n",
      "|            Job_Role|Company|           Location|Job_Experience|Skills_&_Description|\n",
      "+--------------------+-------+-------------------+--------------+--------------------+\n",
      "|Data Scientist: A...|    IBM|Bangalore/Bengaluru|        06-Nov|Scheduling, big d...|\n",
      "|Data Scientist-Cl...|    IBM|             Mumbai|        05-Oct|Consulting, Machi...|\n",
      "|Cognitive/AI Seni...|    IBM|Bangalore/Bengaluru|        04-Aug|Automation, Consu...|\n",
      "|Mid Level Data Sc...|    IBM|          Ahmedabad|        05-Aug|data science, Mac...|\n",
      "|      Data Scientist|    IBM|       Kochi/Cochin|        04-Aug|Software design, ...|\n",
      "|Data Scientist: A...|    IBM|Bangalore/Bengaluru|        02-May|Interpersonal ski...|\n",
      "|Data Scientist: A...|    IBM|Bangalore/Bengaluru|        03-Jul|Data migration, M...|\n",
      "|Data Scientist: A...|    IBM|Bangalore/Bengaluru|        03-Jul|Data migration, M...|\n",
      "|      Data Scientist|    IBM|Bangalore/Bengaluru|        08-Dec|Unix, VMware, Net...|\n",
      "|      Data Scientist|    IBM|       Kochi/Cochin|        08-Dec|Computer science,...|\n",
      "|      Data Scientist|    IBM|Bangalore/Bengaluru|        08-Dec|Consulting, Predi...|\n",
      "|Data Scientist: A...|    IBM|Bangalore/Bengaluru|        04-Aug|IVR, Telecommunic...|\n",
      "|Data Scientist: A...|    IBM|Bangalore/Bengaluru|        03-Jul|Enterprise applic...|\n",
      "|Data Scientist: A...|    IBM|Bangalore/Bengaluru|        04-Aug|IVR, Telecommunic...|\n",
      "|Data Scientist: A...|    IBM|Bangalore/Bengaluru|        03-Jul|Enterprise applic...|\n",
      "|Data Scientist: A...|    IBM|Bangalore/Bengaluru|        03-Jul|Enterprise applic...|\n",
      "|Data Scientist: A...|    IBM|Bangalore/Bengaluru|        03-Jul|Enterprise applic...|\n",
      "|      Data Scientist|    IBM|Bangalore/Bengaluru|        03-Jun|Coding, Debugging...|\n",
      "|      Data Scientist|    IBM|Bangalore/Bengaluru|        03-Jun|Coding, Debugging...|\n",
      "|Data Scientist: A...|    IBM|Bangalore/Bengaluru|        05-Oct|Training, advance...|\n",
      "+--------------------+-------+-------------------+--------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "# SELECT DATA from the naukri_ds_jobs where company place = \"Bangalore\"\n",
    "loc_bang_results = spark.sql(\"SELECT * from upgrad.naukri_ds_jobs where location like '%Bangalore%'\")\n",
    "print(\"Total number of companies having presence in Bangalore is: {}\".format(loc_bang_results.count()))\n",
    "ibm_company_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "52d37c4d-80ea-4fe9-83ed-a974842baedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8abca06196ec4a8a8f15eb5bb069c643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+-----+\n",
      "|            Job_Role|            Job_Role|Day of Month|Month|\n",
      "+--------------------+--------------------+------------+-----+\n",
      "|Data Scientist: A...|Data Scientist: A...|          06|  Nov|\n",
      "|Staff Data Scientist|Staff Data Scientist|          04|  Aug|\n",
      "|Data Scientist-Cl...|Data Scientist-Cl...|          05|  Oct|\n",
      "|Senior Data Scien...|Senior Data Scien...|          05|  Oct|\n",
      "|Cognitive/AI Seni...|Cognitive/AI Seni...|          04|  Aug|\n",
      "|Lead - Data Scien...|Lead - Data Scien...|         Oct|   15|\n",
      "|Sr NLP & Text Min...|Sr NLP & Text Min...|          02|  Jun|\n",
      "|  Sr. Data Scientist|  Sr. Data Scientist|          05|  Oct|\n",
      "|Digital-Senior Pr...|Digital-Senior Pr...|         Aug|   13|\n",
      "|Senior Data Scien...|Senior Data Scien...|          10|  Dec|\n",
      "|      Data Scientist|      Data Scientist|           0|    3|\n",
      "|Senior Associate ...|Senior Associate ...|          02|  Apr|\n",
      "|Data Analyst / Da...|Data Analyst / Da...|          01|  Jun|\n",
      "|Rolls-Royce Data ...|Rolls-Royce Data ...|          03|  May|\n",
      "|      Data Scientist|      Data Scientist|          07|  Oct|\n",
      "|Staff Data Scientist|Staff Data Scientist|          04|  Aug|\n",
      "|      Data Scientist|      Data Scientist|          01|  May|\n",
      "|Senior Data Scien...|Senior Data Scien...|          05| Sept|\n",
      "|Senior Data Scien...|Senior Data Scien...|          05| Sept|\n",
      "|   Sr Data Scientist|   Sr Data Scientist|          04| Sept|\n",
      "+--------------------+--------------------+------------+-----+"
     ]
    }
   ],
   "source": [
    "# Split Job_Experience based on delimitter '-'\n",
    "naukri_df_20 = spark.sql(\"SELECT * from upgrad.naukri_ds_jobs limit 20\")\n",
    "from pyspark.sql.functions import split \n",
    "split_exp_col = split(naukri_df_20['Job_Experience'], '-')\n",
    "splitted_col_df = naukri_df_20.select(\"Job_Role\", \"Job_Role\", split_exp_col.getItem(0).alias('Day of Month'), split_exp_col.getItem(1).alias('Month')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e7d16a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab55e19abc144f4a35f053ab1899979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Job_Exp_Arr|\n",
      "+-----------+\n",
      "|  [06, Nov]|\n",
      "|  [04, Aug]|\n",
      "|  [05, Oct]|\n",
      "|  [05, Oct]|\n",
      "|  [04, Aug]|\n",
      "|  [Oct, 15]|\n",
      "|  [02, Jun]|\n",
      "|  [05, Oct]|\n",
      "|  [Aug, 13]|\n",
      "|  [10, Dec]|\n",
      "|     [0, 3]|\n",
      "|  [02, Apr]|\n",
      "|  [01, Jun]|\n",
      "|  [03, May]|\n",
      "|  [07, Oct]|\n",
      "|  [04, Aug]|\n",
      "|  [01, May]|\n",
      "| [05, Sept]|\n",
      "| [05, Sept]|\n",
      "| [04, Sept]|\n",
      "+-----------+"
     ]
    }
   ],
   "source": [
    "splitted_array_df = spark.sql(\"SELECT split(Job_Experience, '-') as Job_Exp_Arr from upgrad.naukri_ds_jobs limit 20\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "678fc62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760b9eac2c5940ad924e5b4d42e0940f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------+--------------------+\n",
      "|            Job_Role|             Company|            Location|Job_Experience|Skills_&_Description|\n",
      "+--------------------+--------------------+--------------------+--------------+--------------------+\n",
      "|Software Engineer...|              zeotap| Bangalore/Bengaluru|        02-May|Computer science,...|\n",
      "|Data Scientist In...|zMed Healthcare T...|  Chennai, Bengaluru|           0-2|Data analysis, Da...|\n",
      "|      Data Scientist|zCodelabs Softwar...|              Remote|        04-Jul|Artificial Intell...|\n",
      "|Data Scientist In...| zBliss Technologies|  Chennai, Bengaluru|           0-2|Data analysis, Da...|\n",
      "|Data Scientist In...| zBliss Technologies|  Chennai, Bengaluru|           0-1|IT Skills, Python...|\n",
      "|       Data Engineer|xFusion Technologies|             Kolkata|        05-Oct|Java, Data Pipeli...|\n",
      "|  Lead Data Engineer|             whiz.ai|                Pune|        03-Aug|Nosql, Shell Scri...|\n",
      "|    Python Developer|             whiz.ai|                Pune|        02-Jun|Java, Software De...|\n",
      "|Senior Data Engineer|             whiz.ai|                Pune|        06-Oct|Shell Scripting, ...|\n",
      "|Lead Machine Lear...|             whiz.ai|                Pune|       05-Sept|Data Science, Nlp...|\n",
      "|   Python Developers|webtrace software...|           Bengaluru|        02-May|security complian...|\n",
      "|Lead Full Stack D...|             vroutes|Hyderabad/Secunde...|        05-Oct|Css, Full Stack D...|\n",
      "|Product Owner - A...|            vEnrichU|Hyderabad/Secunde...|         15-20|Data42 Platforms,...|\n",
      "|      Data Scientist|vDoIT Technologie...|    Gurgaon/Gurugram|        05-Jul|Computer science,...|\n",
      "|      Senior Analyst|              upGrad|   Mumbai, Bengaluru|        03-Jun|Analytical skills...|\n",
      "|Immediate Hiring ...|              upGrad| Bangalore/Bengaluru|       04-Sept|SQL Queries, Tabl...|\n",
      "|       Data Engineer|              upGrad|           Bengaluru|       04-Sept|Computer science,...|\n",
      "|Urgent Hiring For...|              upGrad|              Mumbai|       04-Sept|Business Intellig...|\n",
      "|      Data Scientist|torcai digital media|                Pune|        03-Jun|Data analysis, da...|\n",
      "|Data Science Engi...|thinkbridge Softw...|              Remote|        01-Apr|Natural Language ...|\n",
      "+--------------------+--------------------+--------------------+--------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "sorted_company_df = spark.sql(\"SELECT * from upgrad.naukri_ds_jobs order by company desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "67b3e06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a9174d13774b50b22701a75a102628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list  of employee data\n",
    "employeeData = [[1, \"susmit\", \"oracle\", 60000],\n",
    "        [2, \"Rahul\", \"microsoft\", 60000], \n",
    "        [3, \"rohith\", \"Meta\", 90000],\n",
    "        [4, \"Aman\", \"amazon\", 80000], \n",
    "        [5, \"bobby\", \"Walmart\", 40000],\n",
    "        [6, \"Sandeep\", \"Walmart\", 50000],\n",
    "        [7, \"Raju\", \"oracle\", 40000]]\n",
    "\n",
    "# Create a schema.\n",
    "employeeSchema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"company\", StringType()),\n",
    "    StructField(\"salary\", IntegerType()),\n",
    "])\n",
    "\n",
    "employeeDF = spark.createDataFrame(employeeData, employeeSchema)\n",
    "employeeDF.write.mode(\"overwrite\").saveAsTable(\"employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a972f99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff05d4f036d4903aa3539fc2e91725b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---------+------+\n",
      "|emp_id|   name|  company|salary|\n",
      "+------+-------+---------+------+\n",
      "|     4|   Aman|   amazon| 80000|\n",
      "|     5|  bobby|  Walmart| 40000|\n",
      "|     6|Sandeep|  Walmart| 50000|\n",
      "|     7|   Raju|   oracle| 40000|\n",
      "|     1| susmit|   oracle| 60000|\n",
      "|     2|  Rahul|microsoft| 60000|\n",
      "|     3| rohith|     Meta| 90000|\n",
      "+------+-------+---------+------+"
     ]
    }
   ],
   "source": [
    "employeeResults = spark.sql(\"select * from employees\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "98c961ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3f22a7a7c84b3ebd5e20ede7fbbd5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write to s3 bucket with partition by company name\n",
    "employeeMaxSalaryByCompanyDf = spark.sql(\"select emp.*, max(salary) over (partition by company) as max_salary from employees emp\")\n",
    "employeeMaxSalaryByCompanyDfWriter = DataFrameWriter(employeeMaxSalaryByCompanyDf)\n",
    "employeeMaxSalaryByCompanyDfWriter.partitionBy('company').saveAsTable('test_table', format='parquet', mode='overwrite', path='s3://kaggle-dataset-pyspark/output_dir/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e349f9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e817c5e3267d4e68876438373c030168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+---------+\n",
      "|emp_id|   name|salary|max_salary|  company|\n",
      "+------+-------+------+----------+---------+\n",
      "|     5|  bobby| 40000|     50000|  Walmart|\n",
      "|     6|Sandeep| 50000|     50000|  Walmart|\n",
      "|     7|   Raju| 40000|     60000|   oracle|\n",
      "|     1| susmit| 60000|     60000|   oracle|\n",
      "|     3| rohith| 90000|     90000|     Meta|\n",
      "|     2|  Rahul| 60000|     60000|microsoft|\n",
      "|     4|   Aman| 80000|     80000|   amazon|\n",
      "+------+-------+------+----------+---------+"
     ]
    }
   ],
   "source": [
    "# Read data back from s3\n",
    "employeesDFFromS3 = spark.read.parquet(\"s3://kaggle-dataset-pyspark/output_dir/\")\n",
    "employeesDFFromS3.createOrReplaceTempView(\"employeesDFFromS3\")\n",
    "employees = spark.sql(\"select * from employeesDFFromS3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bf40ebb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0618264e05af460e8bf05f1c45ec4bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---------+------+\n",
      "|emp_id|   name|  company|salary|\n",
      "+------+-------+---------+------+\n",
      "|     1| susmit|   oracle| 60000|\n",
      "|     2|  Rahul|microsoft| 60000|\n",
      "|     3| rohith|     Meta| 90000|\n",
      "|     4|   Aman|   amazon| 80000|\n",
      "|     5|  bobby|  Walmart| 40000|\n",
      "|     6|Sandeep|  Walmart| 50000|\n",
      "|     7|   Raju|   oracle| 40000|\n",
      "+------+-------+---------+------+"
     ]
    }
   ],
   "source": [
    "# Sort by emp_id asc\n",
    "employeesortById = spark.sql(\"select * from employees order by emp_id asc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b315380b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1ceae4d4824984a3e90df29ef4bbcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-----------+\n",
      "|database|        tableName|isTemporary|\n",
      "+--------+-----------------+-----------+\n",
      "|        |employeesdffroms3|       true|\n",
      "|        |   naukri_ds_jobs|       true|\n",
      "+--------+-----------------+-----------+"
     ]
    }
   ],
   "source": [
    "# Clean up tasks\n",
    "spark.sql(\"drop table if EXISTS upgrad.employees\")\n",
    "spark.sql(\"drop table if EXISTS upgrad.naukri_ds_jobs\")\n",
    "spark.sql(\"drop table if EXISTS upgrad.test_table\")\n",
    "spark.sql(\"drop table if EXISTS upgrad.employeesdffroms3\")\n",
    "spark.sql(\"drop table if EXISTS upgrad.naukri_ds_jobs\")\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "62c65618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9394d13a34a34697b5e1b154020bfb4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"drop database if EXISTS upgrad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7ac693",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
